!SLIDE noprint

!SLIDE smbullets
# Object Storage Daemon (OSD)

<center><img src="./_images/osd.png" style="max-width:80%;height:auto;" alt="osd"/></center>

* one single process managing the data on one single disk
* one server has many OSD processes
* replication and recovery

~~~SECTION:notes~~~
xfs: in production <br/>
btrfs: would be better <br/>
non-fs backend like leveldb are experimental at the moment (Jewel) <br/>
Journal on SSDs or NVRAM <br/>

communicate with each other and report unreachable OSD to MONs<br/>
~~~ENDSECTION~~~


!SLIDE smbullets 
# Monitors
<center><img src="./_images/monitor.png" style="max-width:70%;height:auto;" alt="osd"/></center>

* know the cluster state (clustermap)
* form the quorum (should be an odd number)
* provide information to the client
 * do not provide any data

~~~SECTION:notes~~~
Clustermap: Mon-, OSD-, PG-, Crush- and MDS-Map<br/>
  http://docs.ceph.com/docs/hammer/architecture/#cluster-map<br/>
sends Clustermap to Clients. Essential for CRUSH<br/>
Cluster consensus PAXOS Algorithm (good: small number of participants)<br/>
check as unreachable reported OSDs and change Clustermap accordingly<br/>
~~~ENDSECTION~~~

!SLIDE 
# Ceph Storage Cluster - OSDs + Monitors
<center><img src="./_images/ceph_storage_cluster.png" style="max-width:100%;height:auto;"  alt="Ceph Storage Cluster"/></center>

